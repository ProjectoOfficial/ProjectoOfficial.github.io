<translations>
    <translation lang="it">
        <element id="link-intro">
            <text>Introduzione</text>
        </element>

        <element id="introduction-title">
            <text>Cosa è PyTorch?</text>
        </element>

        <element id="introduction-text">
            <text>
            <a href="https://pytorch.org/">PyTorch</a> è una libreria open-source per machine learning e deep learning sviluppata in origine da Facebook's AI Research lab (FAIR).
            È nota per la sua semplicità d'uso e per la sua capacità di essere uno strumento sintetico per lo sviluppo di reti neurali. Pytorch è disponibile per i principali sistemi operativi come Windows, Linux e MacOS.
            Pytorch è un framework sviluppato sotto forma di libreria per il linguaggio di programmazione Python, ma le funzionalità principali sono scritte in C++ per garantire la massima efficienza. Il codice può
            essere eseguito su CPU e GPU, e la libreria supporta anche il calcolo distribuito. Gli acceleratori hardware attualmente supportati sono CUDA (GPU e GPGPU NVIDIA) e ROCm (GPU Radeon). PyTorch è stato rilasciato 
            per la prima volta nel 2016 ed è stato sviluppato da Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan e Edward Yang.
            </text>
        </element>

        <element id="lecture-0-title">
            <text>Introduzione a PyTorch</text>
        </element>

        <element id="lecture-0-text">
            <text>
            Come tutte le librerie in Python, anche PyTorch va importato utilizzando la classica keyword <code class="inline">import</code>. 
            Come anticipato, PyTorch è in grado di utilizzare diversi dispositivi hardware per la computazione. Uno tra questi è la GPU. 
            Per utenti NVIDIA è sufficiente eseguire <code class="inline">torch.cuda.is_available()</code> per controllare se la GPU è utilizzabile e se i driver sono stati correttamente installati. 
            <code class="inline">torch.device</code> è invece la classe che gestisce qualsiasi dispositivo di computazione, dunque è necessario specificare lì quale si intende utilizzare. 
            Dopodiché le operazioni in pytorch sono tipicamente svolte o tra tensori e tensori, o tra scalari e tensori, dunque è necessario familiarizzare fin da subito con questa struttura dati. 
            <code class="inline">torch.tensor(...)</code> è il metodo che ci permette di creare un tensore. Un tensore in Pytorch è una classe che contiene una serie di attributi fondamentali che 
            definiscono come viene allocato in memoria e come viene letto in modo corretto. La classe tensore ha dunque diversi attributi che ci permettono di profilare il tensore che è stato creato. 
            Oltre a creare un tensore a mano, e possibile creare tensori con valori interni pre-definiti, come tensori composti da tutti i valori a 1, o a 0, oppure tensori composti da numeri randomici.
            </text>
        </element>

        <element id="lecture-1-title">
            <text>Lezione 1 di PyTorch</text>
        </element>

        <element id="lecture-1-text">
            <text>
            Una volta imparato a creare tensori, è necessario anche saperli utilizzare. 
            Tante volte in deep learning è necessario manipolare i tensori, cambiargli aspetto o forma in modo che siano compatibili per le operazioni che devono andare a svolgere. 
            I tensori vengono creati in memoria (RAM o VRAM) in modo contiguo e l'attributo <code class="inline">stride</code>  di ogni tensore descrive scorrerlo. Lo stride indica, per ogni dimensione, 
            il numero di valori consecutivo che costituiscono gli elementi dimensionali del tensore. Ogni tensore ha anche un <code class="inline">dtype</code> che definisce il tipo e il numero di byte che 
            esprimono i valori del tensore. Il prodotto classico svolto tramite l'operatore <code class="inline">*</code> tra due tensori, che <b>devono</b> avere le stesse dimensioni (a meno dell'ultima), 
            è un prodotto element-wise. Stesso discorso vale per gli altri operatori classici. I tensori hanno un attributo <code class="inline">shape</code> che esplicita di quanti elementi è costituita 
            ogni dimensione del tensore. Inoltre ogni tensore ha metodi che a loro volta svolgono operazioni su di esso, come <code class="inline">.sum()</code> 
            che va a sommare tutti i valori presenti in una o più dimensioni specificate. 
            </text>
        </element>

        <element id="lecture-2-title">
            <text>Lezione 2 di PyTorch</text>
        </element>

        <element id="lecture-2-text">
            <text>
            Avanzando con le lezioni andremo ad evidenziare l'importanza di fissare un seed manuale attraverso <code class="inline">torch.manual_seed(42)</code>. 
            Questo ci consente di garantire la riproducibilità tra un esperimento e l'altro. Ciò su cui vogliamo focalizzarci ora sono le automazioni su cui si basa PyTorch, 
            in particolare quelle che avvengono quando svolgiamo operazioni tra tensori. I metodi <code class="inline">.squeeze(dim=-1)</code> e <code class="inline">.unsqueeze(dim=-1)</code> 
            ci permettono di aggiungere una dimensione al tensore nella posizione specificata da <code class="inline">dim</code>. Bisogna ricordare dalla geometria che il prodotto matriciale 
            tra due matrici è possibile soltanto se l'ultima dimensione della prima matrice è uguale alla prima dimensione della seconda, ma in PyTorch c'è una scorciatoia per rendere più lasco questo concetto. 
            Il broadcasting consente di effettuare operazioni aritmetiche tra tensori di forme diverse in modo che le dimensioni delle matrici siano compatibili per il calcolo. Questo avviene 
            senza la necessità di replicare i dati, ma attraverso un processo concettuale che "espande" i tensori in modo che abbiano forme compatibili. La prima regola del broadcasting è che 
            le dimensioni dei tensori vengono confrontate a partire dall'ultima dimensione e procedendo verso la dimensione iniziale; due dimensioni sono compatibili se sono uguali, o se una delle dimensioni è 1. 
            Come funziona il Broadcasting in PyTorch? Se una dimensione di un tensore è 1, il tensore può essere "espanso" lungo quella dimensione così da diventare compatibile all'altro. 
            Se una dimensione di uno dei tensori è mancante, viene assunta come 1 e può essere sottoposta allo stesso procedimento. Il broadcasting non implica la creazione fisica di nuovi tensori; 
            piuttosto, è una vista logica dei dati che consente operazioni efficienti senza duplicazione. Dunque, anche se le dimensioni dei tensori non sono esattamente uguali, possono essere compatibili 
            se le dimensioni sono "broadcastabili" secondo le regole precedentemente indicate. Indexing e slicing sono invece due tecniche fondamentali per accedere e manipolare i dati presenti nei tensori. 
            L'indexing è il processo di accesso a un singolo elemento di una struttura dati utilizzando un indice, mentre lo slicing è il processo di estrazione di una sottosezione di un tensore utilizzando 
            un intervallo di indici. Ad esempio, <code class="inline">x[0, :]</code> accederà al primo elemento della prima dimensione del tensore <i>x</i>, mentre la seconda dimensione viene presa tutta. 
            Questo a meno che non scriviamo un indice <code class="inline">x[0, :30]</code> sulla seconda dimensione che va a limitare il numero di elementi (con indici da 0 a 29 in questo caso). 
            <code class="inline">x[-1]</code> prende l'ultima dimensione del tensore, mentre <code class="inline">x[0, :30, None]</code> è come se applicasse anche un <code class="inline">.unsqueeze(-1)</code> 
            all'operazione descritta prima.
            </text>
        </element>

        <element id="lecture-3-title">
            <text>Lezione 3 di PyTorch</text>
        </element>

        <element id="lecture-3-text">
            <text>
            ToDo
            </text>
        </element>


        <element id="lecture-4-title">
            <text>Lezione 4 di PyTorch</text>
        </element>

        <element id="lecture-4-text">
            <text>
            Una delle cose più importanti da sapere in PyTorch è: come ottimizzare dei parametri. Lo scopo di PyTorch è proprio questo, e per cominciare bisogna definire un numero fisso di epoche, 
            un numero che definisce quante volte l'intero dataset deve essere iterato per ottimizzare i parametri della rete neurale. Un'altro aspetto fondamentale è conoscere la dimensione del 
            proprio input, e definire il numero di parametri di ogni layer della rete neurale. In questo esempio andremo a utilizzare come dati in input alla rete neurale, dei dati generati randomicamente, 
            ma nella lezione 5 su TorchVision, vedremo come utilizzare il dataset MNIST. Bisogna ricordare che creare un tensore in questo modo <code class="inline">torch.randn(N, input_dimension, device=device)</code>, 
            ovvero specificando il device all'interno del metodo, consente di creare il tensore direttamente in GPU (più efficiente). Anche i parametri della rete neurale saranno tensori generati a random, ma in questo 
            caso sarà necessario settare l'attributo <code class="inline">requires_grad=True</code> per abilitare il calcolo del gradiente. Il gradiente, una volta calcolato, sarà un attributo del tensore. 
            La discesa del gradiente è un metodo di ottimizzazione numerica, utilizzato in tanti campi della scienza, Deep Learning incluso. In pratica, la direzione del gradiente punta verso la direzione di minimo 
            di una funzione, in questo caso della funzione di costo. Minimizzare una funzione di costo come una MSE Loss, significa ridurre la distanza tra le predizioni e le ground truth (il nostro obiettivo). 
            Il gradiente non può essere usato da solo, ma è necessario specificare anche l'ampiezza dello step nella direzione del gradiente. Omettere lo step size significa impostarlo ad 1, valore troppo grande 
            che porta l'algoritmo a divergere. Tramite <code class="inline">torch.matmul(input_data, w1)</code> si applicano i "pesi" o i parametri del primo layer della rete all'input, ottenendo un embedding, e 
            stessa cosa si fa con il secondo layer. I valori in output sono confrontati, tramite una MSE loss, con i valori di ground truth, e poi si chiama il metodo <code class="inline">loss.backward()</code> 
            per lanciare il calcolo del gradiente. <code class="inline">with torch.no_grad()</code> è necessario utilizzarlo per evitare che pytorch vada ad allocare spazio per gradienti ulteriori dovuti alle 
            operazioni di aggiornamento dei pesi. Per aggiornare i pesi si sottrae il gradiente moltiplicato per lo step size, poi si resettano i gradienti dei parametri della rete, e si ripete il training loop.
            </text>
        </element>

        <element id="lecture-5-title">
            <text>Lezione 5 di PyTorch</text>
        </element>

        <element id="lecture-5-text">
            <text>
            ToDo
            </text>
        </element>

        <element id="lecture-6-title">
            <text>Lezione 6 di PyTorch</text>
        </element>

        <element id="lecture-6-text">
            <text>
            ToDo
            </text>
        </element>

    </translation>
    

    <translation lang="en">
        <element id="link-intro">
            <text>Introduction</text>
        </element>

        <element id="introduction-title">
            <text>What is PyTorch?</text>
        </element>

        <element id="introduction-text">
            <text>
            <a href="https://pytorch.org/">PyTorch</a> is an open-source library for machine learning and deep learning originally developed by Facebook's AI Research lab (FAIR).
            It is known for its ease of use and its ability to be a synthetic tool for developing neural networks. Pytorch is available for major operating systems such as Windows, Linux, and MacOS.
            Pytorch is a framework developed as a library for the Python programming language, but the core functionality is written in C++ for maximum efficiency. The code can
            run on both CPUs and GPUs, and the library also supports distributed computing. The currently supported hardware accelerators are CUDA (NVIDIA GPUs and GPGPUs) and ROCm (Radeon GPUs). PyTorch was first released in
            2016 and was developed by Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, and Edward Yang.
            </text>
        </element>

        <element id="lecture-0-title">
            <text>Introduction about Pytorch</text>
        </element>

        <element id="lecture-0-text">
            <text>
            Like all Python libraries, PyTorch must also be imported using the classic <code class="inline">import</code> keyword. As anticipated, PyTorch is able to use different hardware devices for computation. 
            One of these is the GPU. For NVIDIA users, it is sufficient to run <code class="inline">torch.cuda.is_available()</code> to check if the GPU is usable and if the drivers have been correctly installed. 
            <code class="inline">torch.device</code> is instead the class that manages any computation device, so it is necessary to specify there which one you intend to use. 
            After that, operations in Pytorch are typically performed either between tensors and tensors, or between scalars and tensors, so it is necessary to immediately familiarize yourself with this data structure. 
            <code class="inline">torch.tensor(...)</code> is the method that allows us to create a tensor. A tensor in Pytorch is a class that contains a series of fundamental attributes that define how it is allocated 
            in memory and how it is read correctly. The tensor class therefore has several attributes that allow us to profile the tensor that has been created. In addition to creating a tensor by hand, 
            it is possible to create tensors with predefined internal values, such as tensors composed of all values to 1, or to 0, or tensors composed of random numbers.
            </text>
        </element>

        <element id="lecture-1-title">
            <text>Lecture 1 about Pytorch</text>
        </element>

        <element id="lecture-1-text">
            <text>
            Once you have learned how to create tensors, you also need to know how to use them. Many times in deep learning it is necessary to manipulate tensors, 
            change their appearance or shape so that they are compatible for the operations they are going to perform. Tensors are created in memory (RAM or VRAM) 
            contiguously and the <code class="inline">stride</code> attribute of each tensor describes how to scroll through it. The stride indicates, for each dimension, 
            the number of consecutive values that constitute the dimensional elements of the tensor. Each tensor also has a <code class="inline">dtype</code> that defines the 
            type and number of bytes that express the values of the tensor. The classic product performed via the <code class="inline">*</code> operator between two tensors, which 
            <b>must</b> have the same dimensions (except for the last one), is an element-wise product. The same goes for the other classic operators. Tensors have a <code class="inline">shape</code> 
            attribute that specifies how many elements each dimension of the tensor is made up of. Each tensor also has methods that perform operations on it, such as <code class="inline">.sum()</code> 
            which adds up all the values in one or more specified dimensions.
            </text>
        </element>

        <element id="lecture-2-title">
            <text>Lecture 2 about Pytorch</text>
        </element>

        <element id="lecture-2-text">
            <text>
            As we move forward with the lessons, we will highlight the importance of setting a manual seed via <code class="inline">torch.manual_seed(42)</code>. 
            This allows us to ensure reproducibility between one experiment and another. What we want to focus on now are the automations that PyTorch relies on, in particular 
            those that occur when we perform operations between tensors. The methods <code class="inline">.squeeze(dim=-1)</code> and <code class="inline">.unsqueeze(dim=-1)</code> 
            allow us to add a dimension to the tensor at the position specified by <code class="inline">dim</code>. It should be remembered from geometry that the matrix product between 
            two matrices is possible only if the last dimension of the first matrix is equal to the first dimension of the second, but in PyTorch there is a shortcut to make this concept more relaxed. 
            Broadcasting allows you to perform arithmetic operations on tensors of different shapes so that the dimensions of the matrices are compatible for computation. 
            This is done without the need to replicate the data, but through a conceptual process that "expands" the tensors so that they have compatible shapes. 
            The first rule of broadcasting is that the dimensions of the tensors are compared starting from the last dimension and proceeding towards the initial dimension; 
            two dimensions are compatible if they are equal, or if one of the dimensions is 1. How does Broadcasting work in PyTorch? If one dimension of a tensor is 1, the tensor can be 
            "expanded" along that dimension so that it becomes compatible with the other. If a dimension of one of the tensors is missing, it is assumed to be 1 and can be subjected to the same process. 
            Broadcasting does not involve the physical creation of new tensors; rather, it is a logical view of the data that allows efficient operations without duplication. So, even if the dimensions of 
            the tensors are not exactly equal, they can be compatible if the dimensions are "broadcastable" according to the rules previously indicated. Indexing and slicing are instead two fundamental 
            techniques to access and manipulate the data present in the tensors. Indexing is the process of accessing a single element of a data structure using an index, while slicing is the process of 
            extracting a subsection of a tensor using a range of indices. For example, <code class="inline">x[0, :]</code> will access the first element of the first dimension of the tensor <i>x</i>, 
            while the second dimension is taken in its entirety. This is unless we write an index <code class="inline">x[0, :30]</code> on the second dimension that limits the number of elements 
            (with indices from 0 to 29 in this case). <code class="inline">x[-1]</code> takes the last dimension of the tensor, while <code class="inline">x[0, :30, None]</code> is as if it also applies an 
            <code class="inline">.unsqueeze(-1)</code> to the operation described above.
            </text>
        </element>

        <element id="lecture-3-title">
            <text>Lecture 3 about PyTorch</text>
        </element>

        <element id="lecture-3-text">
            <text>
            ToDo
            </text>
        </element>

        <element id="lecture-4-title">
            <text>Lecture 4 about Pytorch</text>
        </element>

        <element id="lecture-4-text">
            <text>
            One of the most important things to know in PyTorch is: how to optimize parameters. This is the purpose of PyTorch, and to start you need to define a fixed number of epochs, a number that defines how 
            many times the entire dataset must be iterated to optimize the parameters of the neural network. Another fundamental aspect is to know the size of your input, and define the number of parameters of 
            each layer of the neural network. In this example we are going to use randomly generated data as input data to the neural network, but in lesson 5 on TorchVision, we will see how to use the MNIST dataset. 
            It is important to remember that creating a tensor in this way <code class="inline">torch.randn(N, input_dimension, device=device)</code>, that is, specifying the device inside the method, allows you to 
            create the tensor directly in the GPU (more efficient). The parameters of the neural network will also be randomly generated tensors, but in this case it will be necessary to set the attribute 
            <code class="inline">requires_grad=True</code> to enable the gradient calculation. The gradient, once calculated, will be an attribute of the tensor. Gradient descent is a numerical optimization method, 
            used in many fields of science, including Deep Learning. In practice, the direction of the gradient points towards the minimum direction of a function, in this case the cost function. Minimizing a cost 
            function such as an MSE Loss, means reducing the distance between the predictions and the ground truth (our goal). The gradient cannot be used alone, but it is also necessary to specify the size of the 
            step in the direction of the gradient. Omitting the step size means setting it to 1, a value too large that causes the algorithm to diverge. Using <code class="inline">torch.matmul(input_data, w1)</code> 
            you apply the "weights" or parameters of the first layer of the network to the input, obtaining an embedding, and the same thing is done with the second layer. The output values are compared, via an MSE loss, 
            with the ground truth values, and then the method <code class="inline">loss.backward()</code> is called to launch the gradient calculation. <code class="inline">with torch.no_grad()</code> is necessary to 
            use it to avoid that pytorch allocates space for additional gradients due to the weight update operations. To update the weights you subtract the gradient multiplied by the step size, then you reset the 
            gradients of the network parameters, and repeat the training loop.
            </text>
        </element>

        <element id="lecture-5-title">
            <text>Lecture 5 about PyTorch</text>
        </element>

        <element id="lecture-5-text">
            <text>
            ToDo
            </text>
        </element>

        <element id="lecture-6-title">
            <text>Lecture 6 about PyTorch</text>
        </element>

        <element id="lecture-6-text">
            <text>
            ToDo
            </text>
        </element>

    </translation>

</translations>